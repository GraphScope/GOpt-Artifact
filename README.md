# GOpt Artifacts
This repository contains the artifacts related to the paper titled "A Graph-Native Query Optimization Framework". We offer a comprehensive set of resources to replicate the experiments detailed in the paper. These resources encompass the necessary environment, datasets, and scripts required for executing the experiments.

As delineated in the paper, the experimental analysis is divided into two segments: small-scale experiments and scalability experiments. The small-scale experiments aim to assess GOpt's performance on a single-machine setup using the G30 dataset. Evaluations cover various aspects such as type inference efficiency, the efficacy of heuristic rules, cost-based optimization strategies, and optimization of LDBC queries. In contrast, the scalability experiments focus on examining GOpt's performance across large-scale graphs (ranging from G30 to G1000) within a robust 16-machine cluster environment.

## Small-Scale Experiments

### Settings

#### Hardware
The experiments on a smaller scale were carried out on a single machine powered by dual Intel Xeon E5-2620 v4 CPUs (boasting 8 cores and operating at a 2.1GHz clock speed), 512GB of memory, and a 1TB disk. This setup employs 32 threads for execution.

#### Dataset
For these experiments, we utilized the G30 dataset, which includes 89 million vertices and 541 million edges. This dataset has been preprocessed into a binary format compatible with the GIE system. The entire dataset is approximately 40GB in size and is readily available within the provided docker image. A convenient one-click script is included for loading and testing the dataset.

#### Environment
To facilitate the experiments, a docker image incorporating all necessary binary dependencies, scripts, and the dataset itself is provided. To initiate an experimental container, execute the following command:
```bash
docker run --name=gopt_test -it registry.cn-hongkong.aliyuncs.com/graphscope/graphscope-dev:gopt /bin/bash
```

Within the `/home/graphscope/gopt` directory of the container, you will find:
```bash
├── bin # scripts to run the experiments
│   ├── cbo.sh
│   ├── ldbc.sh
│   ├── rbo.sh
│   └── type_inference.sh
├── configs # configuration files to initiate the GIE system
│   └── ir.compiler.properties
├── data  # dataset
│   └── G30
└── lib # binary dependencies, including some jar files
```

### Evaluation

#### Type Inference
The performance impact of enabling or disabling type inference optimization within the GIE system is assessed. To replicate these experiment results, use the commands below with the --opt with/without flag determining the state of type inference optimization:
```bash
./type_inference.sh --opt with
```
```bash
./type_inference.sh --opt without
```
Expected results are organized as follows:
```
query: [Q_T_1], latency: [32232] ms
query: [Q_T_2], latency: [913] ms
query: [Q_T_3], latency: [63800] ms
...
```
#### Heuristic Rules
This part evaluates different RBO rules' impact on query groups as outlined in the paper. Performance comparisons include FieldTrimRule for [Qr1, Qr2], ExpandGetVFusionRule for [Qr3, Qr4], and FilterIntoMatchRule for [Qr5, Qr6]. To avoid the complexity of the configuration, we have already bound RBO rules to specific query groups in the scripts. You can reproduce the results of the experiments with the following command directly, where `--opt with/without` controls whether to enable the heuristic rules optimization.
```bash
./rbo.sh --opt with
```
```bash
./rbo.sh --opt without
```
The expected output format is as follows:
```
query: [Q_R_1], latency: [117774] ms
query: [Q_R_2], latency: [40629] ms
...
```

#### Cost-based Optimization
In this part of the experiment, we compare the optimal order generated by GOpt and Neo4j on the query set [Qc](https://github.com/alibaba/GraphScope/tree/main/interactive_engine/benchmark/queries/cypher_queries). In addition, we randomly select up to 10 orders for comparison. We have added the `--order GOpt/Neo4j/Random` option in the script to generate the results of different orders:
```bash
./cbo.sh --order GOpt
```
```bash
./cbo.sh --order Random
```
```bash
./cbo.sh --order Neo4j
```
Anticipated outputs should be similar to:
```
*************[Q_C_1_a]*************
plan id [0], latency: [1394331] ms
plan id [1], latency: [23531] ms
plan id [2], latency: [2805822] ms
...
```
When `--order` is specified as GOpt or Neo4j, only one plan id will be output for a query, indicating the optimal execution order generated by GOpt or Neo4j. When `--order` is specified as Random, multiple plan ids will be output for a query, indicating the randomly generated execution order.

#### Optimizing LDBC Queries
To further verify the optimization effect of GOpt in a real scenario, we repeated the above experiments on the [LDBC Query Set](https://github.com/ldbc/ldbc_snb_interactive_v1_impls/tree/main/cypher/queries). Similarly, You can get the performance of LDBC queries on different orders by leveraging the `--order` option:
```bash
./ldbc.sh --order GOpt
```
```bash
./ldbc.sh --order Random
```
```bash
./ldbc.sh --order Neo4j
```
The resultant performance metrics are presented as follows:
```
*************[Q_IC_1]*************
plan id [0], latency: [1927] ms
plan id [1], latency: [532] ms
plan id [2], latency: [4786] ms
...
```

## Scalability Experiments